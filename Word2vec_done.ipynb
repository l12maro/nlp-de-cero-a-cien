{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2vec-done.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/l12maro/nlp-de-cero-a-cien/blob/embeddings/Word2vec_done.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvNCXd9Dfqe1"
      },
      "source": [
        "# Word2vec con Gensim\n",
        "\n",
        "En este cuaderno de Jupyter vas a utilizar la biblioteca [Gensim](https://radimrehurek.com/gensim/index.html) para experimentar con word2vec. Este cuaderno está enfocado en la intuición de los conceptos y no en los detalles de implementación. Este cuaderno está inspirado en esta [guía](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html).\n",
        "\n",
        "## 1. Instalación y cargar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKIqnDXXfpiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f25319-1bfc-4e38-d3b5-cdf708d35451"
      },
      "source": [
        "!pip install --upgrade gensim"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.0.1-cp37-cp37m-manylinux1_x86_64.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 93 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Installing collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7Q2jB3frOn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54cf2e53-e13b-4adb-b9a9-2f84b7c7b6b6"
      },
      "source": [
        "import gensim.downloader as api"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBaT8djWkaZy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e36cd0-4ee1-4223-921e-9de84daefc65"
      },
      "source": [
        "model = api.load('word2vec-google-news-300')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVZm7iTOoawW"
      },
      "source": [
        "## 2. Similitud de palabras\n",
        "\n",
        "En esta sección veremos cómo conseguir la similitud entre dos palabras utilizando un word embedding ya entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOZfaelLoi4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ae2300-6f32-4c36-e9f5-aed1fb57f636"
      },
      "source": [
        "model.similarity(\"king\", \"queen\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6510957"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX-Kk9HZofuF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3994af-c796-4d74-e90a-edc4a7ce4aee"
      },
      "source": [
        "model.similarity(\"king\", \"man\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22942673"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypFK-pLrol3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f40bca8f-faf5-47e8-b9bd-3613b598e9cd"
      },
      "source": [
        "model.similarity(\"king\", \"potato\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09978465"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBWzZySFormq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3a88be-f7e7-416e-8536-8084f156e346"
      },
      "source": [
        "model.similarity(\"king\", \"king\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GijWs_tx83W"
      },
      "source": [
        "Ahora veremos cómo encontrar las palabras con mayor similitud al conjunto de palabras especificado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytELAWBLk2-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b5bd95-fcb4-4064-8fda-740b603382d1"
      },
      "source": [
        "model.most_similar([\"king\", \"queen\"], topn=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('monarch', 0.7042067050933838),\n",
              " ('kings', 0.6780861616134644),\n",
              " ('princess', 0.6731551885604858),\n",
              " ('queens', 0.6679497957229614),\n",
              " ('prince', 0.6435247659683228)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D4ZS7N3ovxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e31cd66-3d19-4bd1-9d0f-1a90d7a693dc"
      },
      "source": [
        "model.most_similar([\"tomato\", \"carrot\"], topn=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('carrots', 0.7536594867706299),\n",
              " ('tomatoes', 0.7129638195037842),\n",
              " ('celery', 0.7025030851364136),\n",
              " ('broccoli', 0.6796350479125977),\n",
              " ('cherry_tomatoes', 0.662927508354187)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZFlxKjOyBpu"
      },
      "source": [
        "Pero incluso puedes hacer cosas interesantes como ver qué palabra no corresponde a una lista."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CrZdcBpn3pn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8abc858c-bedd-4fd9-e658-76ce652a6cf1"
      },
      "source": [
        "model.doesnt_match([\"summer\", \"fall\", \"spring\", \"air\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'air'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko09hZ3dqMZ1"
      },
      "source": [
        "## Ejercicios\n",
        "\n",
        "1. Usa el modelo word2vec para hacer un ranking de las siguientes 15 palabras según su similitud con las palabras \"man\" y \"woman\". Para cada par, imprime su similitud."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzZ1eD3PpT-d"
      },
      "source": [
        "words = [\n",
        "\"wife\",\n",
        "\"husband\",\n",
        "\"child\",\n",
        "\"queen\",\n",
        "\"king\",\n",
        "\"man\",\n",
        "\"woman\",\n",
        "\"birth\",\n",
        "\"doctor\",\n",
        "\"nurse\",\n",
        "\"teacher\",\n",
        "\"professor\",\n",
        "\"engineer\",\n",
        "\"scientist\",\n",
        "\"president\"]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15IV_ohKaMeH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01307d3-bd6a-4c41-c48e-9dafcee8946d"
      },
      "source": [
        "ranking_man = {}\n",
        "ranking_woman = {}\n",
        "for t in words:\n",
        "  ranking_man[t] = model.similarity(\"man\", t)\n",
        "  ranking_woman[t] = model.similarity(\"woman\", t)\n",
        "ranked_man = sorted(ranking_man, key=ranking_man.get, reverse=True)\n",
        "for key in ranked_man: \n",
        "  print(key, ranking_man[key])\n",
        "print(\"-------------------\")\n",
        "ranked_woman = sorted(ranking_woman, key=ranking_woman.get, reverse=True)\n",
        "for key in ranked_woman: \n",
        "  print(key, ranking_woman[key])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man 1.0\n",
            "woman 0.76640123\n",
            "husband 0.34499747\n",
            "wife 0.32920915\n",
            "child 0.31633338\n",
            "doctor 0.31448963\n",
            "nurse 0.2547229\n",
            "teacher 0.25000125\n",
            "king 0.22942673\n",
            "queen 0.16658202\n",
            "scientist 0.15824963\n",
            "engineer 0.15128928\n",
            "birth 0.11078789\n",
            "professor 0.09415862\n",
            "president 0.028424604\n",
            "-------------------\n",
            "woman 1.0\n",
            "man 0.76640123\n",
            "husband 0.49281383\n",
            "child 0.47500372\n",
            "wife 0.444824\n",
            "nurse 0.44135594\n",
            "doctor 0.37945858\n",
            "queen 0.31618136\n",
            "teacher 0.31357846\n",
            "birth 0.21471293\n",
            "scientist 0.15486898\n",
            "professor 0.13077852\n",
            "king 0.12847973\n",
            "engineer 0.09435377\n",
            "president 0.062676705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6b-qOFFAafg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKamywnxqxJJ"
      },
      "source": [
        "**2. Completa las siguientes analogías por tu cuenta (sin usar el modelo)**\n",
        "\n",
        "a. king is to throne as judge is to _\n",
        "\n",
        "b. giant is to dwarf as genius is to _\n",
        "\n",
        "c. French is to France as Spaniard is to _\n",
        "\n",
        "d. bad is to good as sad is to _\n",
        "\n",
        "e. nurse is to hospital as teacher is to _\n",
        "\n",
        "f. universe is to planet as house is to _"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIkFp0-dQVok"
      },
      "source": [
        "Solution 2:\n",
        "\n",
        "a. chair\n",
        "\n",
        "b. stupid / dumb \n",
        "\n",
        "c. Spain\n",
        "\n",
        "d. happy\n",
        "\n",
        "e. school / highschool\n",
        "\n",
        "f. room\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcNRxHuZrXAM"
      },
      "source": [
        "**3. Ahora completa las analogías usando un modelo word2vec**\n",
        "\n",
        "Aquí hay un ejemplo de cómo hacerlo. Puedes resolver analogías como \"A es a B como C es a _\" haciendo A + C - B. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4kF08h4qhxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09b8bb9-81e8-4043-fc10-4982e1326712"
      },
      "source": [
        "# man is to woman as king is to ___?\n",
        "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7118193507194519)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiPbbGsori48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacebe1b-e231-4322-9be2-5107709892d4"
      },
      "source": [
        "# us is to burger as italy is to ___?\n",
        "model.most_similar(positive=[\"Mexico\", \"burger\"], negative=[\"USA\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('taco', 0.6266060471534729)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95qzW2FObvbT",
        "outputId": "0d51e674-2661-4f31-e705-97762e081c62"
      },
      "source": [
        "model.most_similar(positive=[\"throne\", \"judge\"], negative=[\"king\"], topn=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('appellate_court', 0.5845253467559814)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhdUKrCOcQRA",
        "outputId": "1aa5cc47-6b04-4e1f-e048-d68128c3656e"
      },
      "source": [
        "model.most_similar(positive=[\"dwarf\", \"genious\"], negative=[\"giant\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('overated', 0.4708128571510315)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eoj8b6tjcYiB",
        "outputId": "1411bfc5-14c4-49fc-ff72-1f701fbbc214"
      },
      "source": [
        "model.most_similar(positive=[\"France\", \"Spaniard\"], negative=[\"French\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('rider_Dani_Pedrosa', 0.5646752119064331)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2bGl7aVci63",
        "outputId": "bb571b4f-ecdb-42ce-8b68-f2482e738cda"
      },
      "source": [
        "model.most_similar(positive=[\"good\", \"sad\"], negative=[\"bad\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('wonderful', 0.6414927840232849)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daRSor_lcqP4",
        "outputId": "1c4fbbef-e7be-43de-d946-33686b315ad2"
      },
      "source": [
        "model.most_similar(positive=[\"hospital\", \"teacher\"], negative=[\"nurse\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('school', 0.60170978307724)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kW_8e55cxgN",
        "outputId": "3923a2af-a8dd-4adb-e2ac-3435722c2cfa"
      },
      "source": [
        "model.most_similar(positive=[\"planet\", \"house\"], negative=[\"universe\"], topn=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bungalow', 0.5428240299224854)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    }
  ]
}